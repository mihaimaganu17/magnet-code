from __future__ import annotations
from typing import AsyncGenerator, Awaitable, Callable

from magnet_code.agent.events import AgentEvent, AgentEventType
from magnet_code.agent.session import Session
from magnet_code.client.response import (
    StreamEventType,
    StreamEventType,
    TokenUsage,
    ToolCall,
    ToolResultMessage,
)
from magnet_code.config.config import Config
from magnet_code.prompts.system import create_loop_breaker_prompt
from magnet_code.tools.base import ToolConfirmation


class Agent:
    def __init__(
        self,
        config=Config,
        confirmation_callback: Callable[[ToolConfirmation], bool] | None = None,
    ):
        self.config = config
        # Create a new LLM client for this agent that will be used to generate responses
        self.session: Session | None = Session(self.config)
        self.session.approval_manager.confirmation_callback = confirmation_callback

    async def run(self, message: str):
        """Run the agent one time with the given message. The agent yields events for the start of
        the agentic loop, the end of the agentic loop and any potential error in the loop, as well
        as the text delta progress and the completion of a message."""
        await self.session.hook_system.trigger_before_agent(user_message=message)
        # The first event in an agent's run is communicating back that the agent has started
        yield AgentEvent.agent_start(message)

        # Add the user message to the context
        self.session.context_manager.add_user_message(message)
        # Future add-ons:
        #   agent hooks that could run

        final_response: str | None = None
        # Run the main agentic loop
        async for event in self._agentic_loop():
            yield event

            if event.type == AgentEventType.TEXT_COMPLETE:
                final_response = event.data.get("content")

        await self.session.hook_system.trigger_after_agent(message, final_response)
        # After the loop completed, communicate it with a final end event
        yield AgentEvent.agent_end(final_response)

    async def _agentic_loop(self) -> AsyncGenerator[AgentEvent, None]:
        """
        Agentic loop with:
        - multi-turn conversation
        - context management (coming soon)
        """
        max_turns = self.config.max_turns

        for turn_num in range(max_turns):
            # Keeping track of turns in the session
            self.session.increment_turn()
            # Initial variable where we accumulate the response from the LLM
            response_text = ""
            summary = None

            # Check for context overflow
            if self.session.context_manager.needs_compression():
                summary, usage = await self.session.chat_compactor.compress(
                    self.session.context_manager
                )

            if summary:
                self.session.context_manager.replace_with_summary(summary)
                self.session.context_manager.set_latest_usage(usage)
                self.session.context_manager.add_usage(usage)

            # Get OpenAI API compatible schema of all the tools in the registry, such that we can add
            # them to the LLM request and the LLM knows which tools are available for calling
            tool_schemas = self.session.tool_registry.get_schemas()
            # Keep track of the list of tool calls the LLM sends back as response
            tool_calls: list[ToolCall] = []
            usage: TokenUsage | None = None

            # Issue a chat completion request to the LLM client and handle the yielded events
            async for event in self.session.client.chat_completion(
                self.session.context_manager.get_messages(),
                tools=tool_schemas if tool_schemas else None,
                stream=True,
            ):
                # If the stream event is a text delta (a new token generated by the LLM)
                if event.type == StreamEventType.TEXT_DELTA:
                    if event.text_delta:
                        # We convert it to the delta agent event and accumulate the content
                        content = event.text_delta.content
                        response_text += content
                        # And yield a new `AgentEvent` for content progress
                        yield AgentEvent.text_delta(content)
                # If the client successfuly assembled the tool call, we add it to our tool_calls list
                elif event.type == StreamEventType.TOOL_CALL_COMPLETE:
                    if event.tool_call:
                        # TODO: Does this work for the non-stream response as well?
                        tool_calls.append(event.tool_call)
                # If it is an error report an agent error event as well.
                elif event.type == StreamEventType.ERROR:
                    yield AgentEvent.agent_error(
                        event.error or "Unknown error occurred."
                    )
                elif event.type == StreamEventType.MESSAGE_COPLETE:
                    usage = event.usage

            # Add the response as an asssitant message
            self.session.context_manager.add_assistant_message(
                response_text or None,
                (
                    [
                        {
                            "id": tc.call_id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments_delta),
                            },
                        }
                        for tc in tool_calls
                    ]
                    if tool_calls
                    else None
                ),
            )
            # After processing all events, if we have a text response, issue a `AgentEvent` to show the
            # complete text response accumulated
            if response_text:
                yield AgentEvent.text_complete(response_text)
                self.session.loop_detector.record_action("response", text=response_text)

            if not tool_calls:
                if usage:
                    self.session.context_manager.set_latest_usage(usage)
                    self.session.context_manager.add_usage(usage)

                pruned_counts = self.session.context_manager.prune_tool_outputs()
                return

            # Keeping track of the results of the tools associated with their ID, such that we can
            # refeed them to the next LLM request through the context manager.
            tool_call_results: list[ToolResultMessage] = []
            # Execute tool calls
            for tool_call in tool_calls:
                # Tell the caller we have started a tool call execution
                yield AgentEvent.tool_call_start(
                    tool_call.call_id,
                    tool_call.name,
                    tool_call.arguments_delta,
                )
                self.session.loop_detector.record_action(
                    "tool_call",
                    tool_name=tool_call.name,
                    args=tool_call.arguments_delta,
                )

                

                # Invoke the tool
                result = await self.session.tool_registry.invoke(
                    tool_call.name,
                    tool_call.arguments_delta,
                    self.config.cwd,
                    self.session.hook_system,
                    self.session.approval_manager,
                )

                # Issue a completed tool call event
                yield AgentEvent.tool_call_complete(
                    tool_call.call_id,
                    tool_call.name,
                    result,
                )

                # Store the tool call results
                tool_call_results.append(
                    ToolResultMessage(
                        tool_call.call_id,
                        content=result.to_model_output(),
                        is_error=not result.success,
                    )
                )

            # Add all the tool execution results to the context manager which will be used in the next
            # request to the LLM.
            for tool_result in tool_call_results:
                self.session.context_manager.add_tool_result(
                    tool_result.tool_call_id,
                    tool_result.content,
                )

            loop_detected = self.session.loop_detector.check_for_loop()
            if loop_detected:
                # TODO: How are we detecting this?
                loop_prompt = create_loop_breaker_prompt(loop_detected)
                self.session.context_manager.add_user_message(loop_prompt)
                # TODO: Add a new TUI event to handle that
                print(loop_detected)
                continue

            if usage:
                self.session.context_manager.set_latest_usage(usage)
                self.session.context_manager.add_usage(usage)

            pruned_counts = self.session.context_manager.prune_tool_outputs()

        # At this point we have have finished the agentic loop without fulfilling the goal and the
        # max turns have been reached
        yield AgentEvent.agent_error(f"Maximum turns ({max_turns}) reached")

    async def __aenter__(self) -> Agent:
        """Python helper function to open a context handler used by `with` statements"""
        await self.session.initialize()
        return self

    async def __aexit__(self, exc_type, exc_value, traceback) -> None:
        """Python helper function to close a context handler used by `with` statements"""
        if self.session and self.session.client:
            await self.session.client.close()
            self.session = None

        if self.session and self.session.mcp_manager:
            await self.session.mcp_manager.shutdown()
