from __future__ import annotations
from pathlib import Path
from typing import AsyncGenerator

from magnet_code.agent.events import AgentEvent, AgentEventType
from magnet_code.client.llm_client import LLMClient
from magnet_code.client.response import (
    StreamEventType,
    StreamEventType,
    ToolCall,
    ToolResultMessage,
)
from magnet_code.config.config import Config
from magnet_code.context.manager import ContextManager
from magnet_code.tools.builtin.registry import create_default_registry


class Agent:
    def __init__(self, config=Config):
        self.config = config
        # Create a new LLM client for this agent that will be used to generate responses
        self.client = LLMClient(config)
        self.context_manager = ContextManager(config)
        self.tool_registry = create_default_registry()

    async def run(self, message: str):
        """Run the agent one time with the given message. The agent yields events for the start of
        the agentic loop, the end of the agentic loop and any potential error in the loop, as well
        as the text delta progress and the completion of a message."""
        # The first event in an agent's run is communicating back that the agent has started
        yield AgentEvent.agent_start(message)

        # Add the user message to the context
        self.context_manager.add_user_message(message)
        # Future add-ons:
        #   agent hooks that could run

        final_response: str | None = None
        # Run the main agentic loop
        async for event in self._agentic_loop():
            yield event

            if event.type == AgentEventType.TEXT_COMPLETE:
                final_response = event.data.get("content")

        # After the loop completed, communicate it with a final end event
        yield AgentEvent.agent_end(final_response)

    async def _agentic_loop(self) -> AsyncGenerator[AgentEvent, None]:
        """
        Agentic loop with:
        - multi-turn conversation
        - context management (coming soon)
        """
        max_turns = self.config.max_turns
        
        for turn_num in range(max_turns):
            # Initial variable where we accumulate the response from the LLM
            response_text = ""

            # Get OpenAI API compatible schema of all the tools in the registry, such that we can add
            # them to the LLM request and the LLM knows which tools are available for calling
            tool_schemas = self.tool_registry.get_schemas()
            # Keep track of the list of tool calls the LLM sends back as response
            tool_calls: list[ToolCall] = []

            # Issue a chat completion request to the LLM client and handle the yielded events
            async for event in self.client.chat_completion(
                self.context_manager.get_messages(),
                tools=tool_schemas if tool_schemas else None,
                stream=True,
            ):
                # If the stream event is a text delta (a new token generated by the LLM)
                if event.type == StreamEventType.TEXT_DELTA:
                    if event.text_delta:
                        # We convert it to the delta agent event and accumulate the content
                        content = event.text_delta.content
                        response_text += content
                        # And yield a new `AgentEvent` for content progress
                        yield AgentEvent.text_delta(content)
                # If the client successfuly assembled the tool call, we add it to our tool_calls list
                elif event.type == StreamEventType.TOOL_CALL_COMPLETE:
                    if event.tool_call:
                        # TODO: Does this work for the non-stream response as well?
                        tool_calls.append(event.tool_call)
                # If it is an error report an agent error event as well.
                elif event.type == StreamEventType.ERROR:
                    yield AgentEvent.agent_error(event.error or "Unknown error occurred.")

            # Add the response as an asssitant message
            self.context_manager.add_assistant_message(
                response_text or None,
                (
                    [
                        {
                            "id": tc.call_id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments_delta),
                            },
                        }
                        for tc in tool_calls
                    ]
                    if tool_calls
                    else None
                ),
            )
            # After processing all events, if we have a text response, issue a `AgentEvent` to show the
            # complete text response accumulated
            if response_text:
                yield AgentEvent.text_complete(response_text)

            if not tool_calls:
                return
            
            # Keeping track of the results of the tools associated with their ID, such that we can
            # refeed them to the next LLM request through the context manager.
            tool_call_results: list[ToolResultMessage] = []
            # Execute tool calls
            for tool_call in tool_calls:
                # Tell the caller we have started a tool call execution
                yield AgentEvent.tool_call_start(
                    tool_call.call_id,
                    tool_call.name,
                    tool_call.arguments_delta,
                )

                # Invoke the tool
                result = await self.tool_registry.invoke(
                    tool_call.name,
                    tool_call.arguments_delta,
                    self.config.cwd,
                )

                # Issue a completed tool call event
                yield AgentEvent.tool_call_complete(
                    tool_call.call_id,
                    tool_call.name,
                    result,
                )

                # Store the tool call results
                tool_call_results.append(
                    ToolResultMessage(
                        tool_call.call_id,
                        content=result.to_model_output(),
                        is_error=not result.success,
                    )
                )

            # Add all the tool execution results to the context manager which will be used in the next
            # request to the LLM.
            for tool_result in tool_call_results:
                self.context_manager.add_tool_result(
                    tool_result.tool_call_id,
                    tool_result.content,
                )

    async def __aenter__(self) -> Agent:
        """Python helper function to open a context handler used by `with` statements"""
        return self

    async def __aexit__(self, exc_type, exc_value, traceback) -> None:
        """Python helper function to close a context handler used by `with` statements"""
        if self.client:
            await self.client.close()
            self.client = None
