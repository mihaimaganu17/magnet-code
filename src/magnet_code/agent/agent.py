from __future__ import annotations
from typing import AsyncGenerator

from magnet_code.agent.events import AgentEvent, AgentEventType
from magnet_code.client.llm_client import LLMClient
from magnet_code.client.response import StreamEventType, StreamEventType
from magnet_code.context.manager import ContextManager
from magnet_code.tools.builtin.registry import create_default_registry


class Agent:
    def __init__(self):
        # Create a new LLM client for this agent that will be used to generate responses
        self.client = LLMClient()
        self.context_manager = ContextManager()
        self.tool_registry = create_default_registry()

    async def run(self, message: str):
        """Run the agent one time with the given message. The agent yields events for the start of
        the agentic loop, the end of the agentic loop and any potential error in the loop, as well
        as the text delta progress and the completion of a message."""
        # The first event in an agent's run is communicating back that the agent has started
        yield AgentEvent.agent_start(message)

        # Add the user message to the context
        self.context_manager.add_user_message(message)
        # Future add-ons:
        #   agent hooks that could run

        final_response: str | None = None
        # Run the main agentic loop
        async for event in self._agentic_loop():
            yield event

            if event.type == AgentEventType.TEXT_COMPLETE:
                final_response = event.data.get("content")

        # After the loop completed, communicate it with a final end event
        yield AgentEvent.agent_end(final_response)

    async def _agentic_loop(self) -> AsyncGenerator[AgentEvent, None]:
        """
        Agentic loop with:
        - multi-turn conversation
        - context management (coming soon)
        """
        # Initial variable where we accumulate the response from the LLM
        response_text = ""

        tool_schemas = self.tool_registry.get_schemas()

        # Issue a chat completion request to the LLM client and handle the yielded events
        async for event in self.client.chat_completion(
            self.context_manager.get_messages(),
            tools=tool_schemas if tool_schemas else None,
            stream=True,
        ):
            # If the stream event is a text delta (a new token generated by the LLM)
            if event.type == StreamEventType.TEXT_DELTA:
                if event.text_delta:
                    # We convert it to the delta agent event and accumulate the content
                    content = event.text_delta.content
                    response_text += content
                    # And yield a new `AgentEvent` for content progress
                    yield AgentEvent.text_delta(content)
            # If it is an error report an agent error event as well.
            elif event.type == StreamEventType.ERROR:
                yield AgentEvent.agent_error(event.error or "Unknown error occurred.")

        # Add the response as an asssitant message
        self.context_manager.add_assistant_message(response_text or None)
        # After processing all events, if we have a text response, issue a `AgentEvent` to show the
        # complete text response accumulated
        if response_text:
            yield AgentEvent.text_complete(response_text)

    async def __aenter__(self) -> Agent:
        """Python helper function to open a context handler used by `with` statements"""
        return self

    async def __aexit__(self, exc_type, exc_value, traceback) -> None:
        """Python helper function to close a context handler used by `with` statements"""
        if self.client:
            await self.client.close()
            self.client = None
